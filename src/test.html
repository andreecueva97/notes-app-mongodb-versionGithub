<!DOCTYPE HTML>
<html>
 
<head>
  <title>DeepLearning</title>
  <meta name="description" content="website description" />
  <meta name="keywords" content="website keywords, website keywords" />
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
  <link rel="stylesheet" type="text/css" href="css/style.css" />
  <!-- modernizr enables HTML5 elements and feature detects -->
  <script type="text/javascript" src="js/modernizr-1.5.min.js"></script>
  <script src="Scripts/AC_RunActiveContent.js" type="text/javascript"></script>
  <style type="text/css">
	  table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td, th {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

tr:nth-child(even) {
  background-color: #dddddd;
}
<!--
.style1 {color: #FFFFFF}
.style2 {color: #6FA5E1}
.style7 {
	color: #0dbbd5;
	font-size: 36px;
}
-->
  </style>
	
</head>
 
<body>
<div id="main">
    <header>
      <div id="logo">
        <div id="logo_text">
          <!-- class="logo_colour", allows you to change the colour of the text -->
          <h1><a href="index.html">Deep Learning de<span class="logo_colour"> </span></a>San Emigdio Especialista Machine learning </h1>
          <h2></h2>
        </div>
      </div>
      <nav>
        <div id="menu_container">
          <ul class="sf-menu" id="nav">
             <li><a href="index.html">Inicio</a></li>
            <li><a href="valores.html">Valores Marginales</a></li>
 
			<li><a href="sanemigdio.html" class="sf-menu" id="nav">San Emigdio</a>
  <ul>
    <li><a href="ubicaciongeografica.html">Ubicacion Geografica</a></li>
    <li><a href="patrimonio.html">Patrimonio Naturales</a></li>
    <li><a href="actividadesculturales.html">Actividades Culturales</a></li>
	<li><a href="turismo.html">Turismo</a></li>
	<li><a href="galeria.html">Galeria</a></li>
  </ul>
</li>
 
 
 
 
			<li><a href="gestionesmunisipales.html" class="sf-menu" id="nav">Gestiones Munisipales</a>
 
			<ul>
    <li><a href="obras.html">Obras</a></li>
    <li><a href="obras sociales.html">Obras Sociales</a></li>
	<li><a href="proyectosmuniscipales.html">Proyectos Munisipales</a></li>
  </ul>
</li>
 
 
            <li><a href="#">Contactenos</a>
              <ul>
                <li><a href="pterm1.html">Correo Institucional</a></li>
                <li><a href="pterm2.html">Facebook.</a></li>
                <li><a href="pterm3.html">Twitter.</a></li>
              </ul>
            </li>
          </ul>
        </div>
      </nav>
    </header>
    <div id="site_content">
      <table width="952" height="656" border="0">
        <tr>
          <th width="236" scope="col"><p class="style2"><img src="images/Imagen1.jpg" width="230" height="270"></p>
          <p class="style2">&nbsp;</p>
          <p class="style2">&nbsp;</p>
          <p class="style2">&nbsp;</p>
          <p class="style2">&nbsp;</p>
          </th>
          <th width="453" height="652" scope="col"><h1 align="center" class="style7">Introduccion al Machine Learning </h1>
            <p align="justify" class="style1"><strong>Queridos Amigos: </strong><br>
            <p align="justify" class="style1">Reciban un afectuoso saludo de  parte de su servidor Javier Antonio Pérez. Para mí es un verdadero honor  presidir al Concejo Municipal del Pueblo de San Emigdio. Departamento de La  Paz, y poder trabajar para todos ustedes de la mano, persiguiendo el desarrollo  local de nuestro querido pueblo. <br>
              Siempre he considerado que  la mejor manera de tener éxito en la vida. Es por medio del dialogo, de la mano  de las personas, de escuchar a los habitantes, de pensar en el bienestar de los  demás, por esta razón mi administración municipal será bajo el mismo enfoque,  un gobierno cercano a la gente. <br>
              Mi persona y Concejo  Municipal estamos comprometidos con todos ustedes por lo que gestionaremos y  llevaremos a cabo obras en las diferentes comunidades de nuestro querido  municipio, las cuales vendrán a mejorar las condiciones de vida de cada uno de  ustedes.<br>
  <br>
              Tenemos la confianza puesta  en Dios quien nos guiará en la mejor toma de decisiones ya que estas van  encaminadas para poder seguir trabajando para el desarrollo integral de nuestro  querido pueblo, para todos los que en el habitamos y todos los que nos visitan,  porque estamos iniciando un verdadero cambio en beneficio de todos y todas.</p>
            <p align="justify" class="style1"><br>
            Que Dios nos bendiga a  todos. </p>
            <p class="style1"><strong>Javier Antonio Pérez.</strong><br>
                        <strong>Alcalde Municipal de San  Emigdio</strong><br>
                        <strong>Depto. La Paz.   </strong></p>
            <p class="style1">&nbsp;</p>
            <p class="style1">&nbsp;</p>          </th>
          <th width="249" scope="col"><p align="right"><img src="images/ku.png" alt="k" width="243" height="267"></p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
          </th>
        </tr>
      </table>
      <div align="justify"></div>
	  <table width="2000">
  <tr>
    <th>Company 0 ML</th>
    
  </tr>
  <tr>
	  
	  
	  
	  
	  <td><pre>
	matsplot.py
	import numpy as np
"""
Completar las funciones señaladas con la logica
correspondiente segun conceptos vistos en clase 0. 
No remover ni modifcar la constante X
"""
X = np.arange(10)

def y_values(x, mode):
    """
    Given an array of X values, return the y
    values according to the mode parameter.
    mode can be l for a linear function y = m*x + b
    where b = 2 and m = 3 and q for a quadratic function
    y = x^2
    >>> y_values(X, 'l')
    array([ 2,  5,  8, 11, 14, 17, 20, 23, 26, 29])
    >>> y_values(X, 'q')
    array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])
    """
    pass
	
	def plot_x_y(x,y, mode):
    """
    Given x and y values of the same size, use matplot 'plt'
    to plot a specific chart. This function can be solve in one line
    """
    pass
	
	if __name__ == '__main__':
    #a sample result, try as many as you wany
    Y = y_values(X, 'l')
    plot_x_y(X, Y, 'r')
    plt.show()
</pre></td>

  
  </tr>
  <tr>
    <td><pre>
	panda.py
	import pandas as pd
"""
Completar las funciones señaladas con la logica
correspondiente segun conceptos vistos en clase 0. 
No modificar la constante SRC. Todas las funciones
se resuelven en una o dos lineas de codigo
"""
SRC = {"Name":
           ["Geoffrey Hinton",
            "Michael I Jordan",
            "Andrew Ng",
            "Yann LeCun",
            "Yoshua Bengio"],
       "Age": [73, 65, 45, 61, 57],
       "Country":
           ["UK", "US", "UK", "FR", "FR"]}


def set_dataframe(source):
    """
    From the SRC constant, create a panda data frame
    and return it. This will be a useful helper function
    """
    pass


def average_age(data_frame):
    """
    Given a data frame, get the average
    age of all of the names in list
    >>> average_age( set_dataframe(SRC) )
    60.2
    """
    pass


def people_from(a_country, data_frame):
    """
    Given a data frame, get all the people
    from a given country
    """
    pass

if __name__ == '__main__':
    data = set_dataframe(SRC)
    print(data)
    print("Average Age:", average_age(data))
    print(people_from("UK", data))
	</pre></td>
   
  </tr>
  <tr>
    <td><pre>
	warm-up.py
"""
Completar las siguientes funciones definidas de modo que cumplan
con la logica solicitada. El proposito de este ejercicio es recordar
elementos de Python (req. Informatica General). Se asignan tests previamente
definidos (no es necesario agregar nuevos casos).
"""


def list_has_even_size(lst):
    """
    Given a list return True if its size
    is an even number, otherwise False

    >>> list_has_even_size([1, 2, 3])
    False
    >>> list_has_even_size(['a', 'b', 'c', 'd'])
    True
    >>> list_has_even_size([])
    True
    """
    pass


def sum_of_elements(lst):
    """
    Given a list with integer numbers
    return the sum of all the elements.
    >>> sum_of_elements([1, 25, 45, 30])
    101
    >>> sum_of_elements([5, 3, -1])
    7
    >>> sum_of_elements([])
    0
    >>> sum_of_elements([-5, -2])
    -7
    """
    pass


def remove_elements(array):
    """
    Given a 2d array, return an array
    that contains only the rows that have not
    None as an element
    >>> remove_elements([[1, 2], [3, 4]])
    [[1, 2], [3, 4]]
    >>> remove_elements([[1, None], [2, 3]])
    [[2, 3]]
    >>> remove_elements([[1, None], [1, None]])
    []
    """
    pass


def replace_value(array):
    """
    Given a 2d array, replace every 'x'/'X' character
    with an 'o'/'O' character.
    >>> replace_value([['a', 'x'], ['o', 'b']])
    [['a', 'o'], ['o', 'b']]
    >>> replace_value([['a', 'b'], ['c', 'd']])
    [['a', 'b'], ['c', 'd']]
    >>> replace_value([['X', 'x'], ['xx', 'XX']])
    [['O', 'o'], ['xx', 'XX']]
    """
    pass
		  </pre></td>
   
  </tr>
  
  <tr>
    <td></td>
    
  </tr>
<tr>
    <th>Company 1 ML</th>
    <td><pre>

1           regresion lineal


<h4>Regresion Lineal Simple</h4>

Vamos a empezar con lo que verdaderamente nos trae a esta practica: Modelos de Regresion.

Comenzaremos con un modelo de Regresion Lineal Simple. Consideraciones:
- Trabajaremos con un Dataset que incluye datos de las propiedades de Boston
- La libreria a usar es scikit-learn (https://jmlr.csail.mit.edu/papers/volume12/pedregosa11a/pedregosa11a.pdf)
- Usaremos matplot para visualizar la data.



#Importamos la libreria sklearn, una libreria sobre machine learning en Python. 
#De estas librerias traemos datasets,que son datos que vienen con la libreria 
#para fines educativos. Traeremos el modelo de regresion lineal que vamos a entrenar.
from sklearn import datasets, linear_model
#De la libreria sklearn.metrics traeremos aqullas formulas que nos permiten 
#evaluar la precision de las predicciones
from sklearn.metrics import mean_squared_error, r2_score



Nuestro dataset sera un conjunto de datos sobre viviendas en Boston. (https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset)
Una cosa a tener en cuenta: sklearn viene con los datasets de Boston cargados y su manipulacion para entrenar el modelo es trivial. Pero nosotros no vamos a hacer las cosas tan a alto nivel. Vamos a usar lo aprendido anteriormente para que manipulen los datos desde 0. Por ellos vamos a construir nosotros nuestro DF con panda



#Antes de comenzar, recomiendo leer las referencias del dataset
import pandas as pd
boston = pd.read_csv('boston.csv')
#Veamos los primeros elementos
boston.head()
#Nuestro dataset tiene 13 variables explicativas (CRIM, ZN, INDUS, CHAS, ..., LSTAT)
#La variable objetivo es MEDV y significa Median value of owner-occupied homes in $1000’s




var_explicativas = boston[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]
var_explicativas.head()


var_objetivo = boston[['MEDV']]
var_objetivo.head()



import matplotlib.pyplot as plt

loop

variables = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
for var in variables:
    x = boston[[var]]
    plt.scatter(x, var_objetivo)
    plt.xlabel(var)
    plt.ylabel("MEDV")
    plt.show()
	
	
	Entrenemos un modelo de regresion lineal simple. Nuestro objetivo es predecir el precio de una vivienda en funcion de LSTAT.
	
	
	#Paso 1: Cargamos el modelo
regr = linear_model.LinearRegression()



#La variable explicativa sera la columna Rooms
var_explicativa = boston[['RM']]
#Paso 2: (es clave) entrenar el modelo en base a los datos fuente
regr.fit(var_explicativa, var_objetivo)

pendiente positiva tiene sentido??

#Veamos cual es el beta 1 de este modelo. (la pendiente)
beta_1 = regr.coef_
beta_1

variable independiente negatvia tiene sentido??


beta_0 = regr.intercept_
beta_0


	prediccion_precios = regr.predict(var_explicativa)
prediccion_precios


#Grafiquemos la linea de regresion y la dispersion de datos
plt.scatter(var_explicativa, var_objetivo)
plt.plot(var_explicativa, prediccion_precios, 'r')
plt.xlabel('LSTAT')
plt.ylabel('MEDV')



import numpy as np
precio = np.transpose(var_objetivo)

for index, row in var_objetivo.iterrows():
    print(index, row)
	
	
	
	
	#Calculemos el error cuadrado medio
sce = 0
for i in range(0, precio.size):
    sce += np.power(precio[i] - prediccion_precios[i], 2)
mse = sce / precio.size
mse



#Cuanto es el MSE o Error Cuadrado Medio
mean_squared_error(var_objetivo, prediccion_precios)




#Cuanto es el R2
r2_score(var_objetivo, prediccion_precios)



<h4>Regresion Lineal Multiple</h4>

Ahora entrenaremos un modelo de regresion que se explique por dos variables: LSTAT y RM. Para ello utilizaremos tambien regresion lineal



#En este caso elegiremos dos columnas como variables explicativas
var_explicativa = boston[['LSTAT', 'RM', 'NOX']]
var_explicativa.head()



ax = plt.figure().add_subplot(projection='3d')
ax.scatter(boston['LSTAT'], boston['RM'], boston['MEDV'])
ax.set_xlabel('LSTAT')
ax.set_ylabel('ROOMS')
ax.set_zlabel('MEDV')


#Entrenamos el modelo, la var_objetivo sigue siendo MEDV (declarada previamente)
regr.fit(var_explicativa, var_objetivo)




#Veamos cuales son los coeficientes Beta 1 y Beta 2
regr.coef_



#Y ahora cual es Beta 0
regr.intercept_




#Veamos que resultados da el modelo
prediccion_prec = regr.predict(var_explicativa)
prediccion_prec



#Calculemos el error cuadrado medio
sce = 0
for i in range(0, precio.size):
    sce += np.power(precio[i] - prediccion_precios[i], 2)
mse = sce / precio.size
mse



mean_squared_error(var_objetivo, prediccion_precios)



#Cuanto es el R2
r2_score(var_objetivo, prediccion_precios)




<h4>Calculemos $R^{2}$ Ajustado</h4>

Segun lo visto en clase, definimos a $R^{2}$ como:

$R^{2} = 1 - \frac{RSS}{TSS}$

donde RSS es Residual Sum of Squares y se calcula como:

$RSS = \Sigma(y_{i} - Y_{i})^{2}$

y TSS:

$TSS = \Sigma(y_{i} - y_{med-i})^{2}$

Para $R^{2}_{ajus}$ queremos penalizar la incorporacion de nuevas variables:

$R^{2}_{ajus} = 1 - \frac{RSS / (n - d -1)}{TSS / (n-1)}$

remplazando terminos:

$R^{2}_{ajus} = 1 - (1 - R^{2})(\frac{n-1}{n-d-1})$






#En base a lo definido anteriormente calculemos R2 ajustado
#N es el numero de ocurrencias
n = var_objetivo.size
#d es el numero de variables explicativas
d = var_explicativas.columns.size
print('n: ', n, 'd: ', d)




#Planteamos nuestra formula y veamos el resultado
r2_ajust = 1 - (1 - r2) * ((n - 1) / (n - d - 1))
print('R2 Ajustado:', round(r2_ajust, 3))
print('R2: ', round(r2, 3))



</pre></td>
  </tr>
 		
  <tr>
    <th>Company 2 ML</th>
    
  </tr>
practica 2
<td><pre>
<h4>técnicas de selección discutidas la última clase: Forward Stepwise Selection, Backward Stepwise Selection y Backward Stepwise Selection with p-values.
</h4>


#Importamos las librerias que vamos a usar hoy
import pandas as pd
import numpy as np
from sklearn import linear_model
from sklearn.metrics import r2_score
import statsmodels.api as sm
import operator
import matplotlib.pyplot as plt





#Traemos el dataset de Boston y armaremos nuestro modelo con todas las variables
boston = pd.read_csv('boston.csv')
var_explicativas = boston.drop('MEDV', 1)
var_objetivo = boston[['MEDV']]





<h4>Una nueva libreria: StatsModels</h4>

Esta libreria permite hacer estimaciones para diferentes modelos estadísticos, además de permitir hacer pruebas estadísticas y exploración de data https://www.statsmodels.org/stable/index.html.

Respecto a SKLearn, para Regresión Lineal las diferencias no son significativas. Podemos leer este artículo donde se comparan ambos módulos: https://becominghuman.ai/stats-models-vs-sklearn-for-linear-regression-f19df95ad99b 

¿Por que lo utilizaremos? porque nos brinda muchos detalles estadisticos que SKLearn no.







#En StatsModels debemos agregar nosotros mismo la constante para el armado de nuestro modelo.
var_explicativas = sm.add_constant(var_explicativas)
var_explicativas.head()





#Definimos nuestro modelo, OLS significa Ordinary Least Squares. Esto indica la forma en la que
#se calcularan los parametros
model = sm.OLS(var_objetivo, var_explicativas)





#Entrenamos el modelo (notemos las diferencias con sklearn)
regr = model.fit()





#Veamos todos los datos que nos entrega StatsModels
print(regr.summary())





#Para acceder al R2 y R2 Ajustado
r2 = regr.rsquared
r2_adj = regr.rsquared_adj
print('R2: ', round(r2, 3))
print('R2 Ajustado:', round(r2_adj, 3))







#Y lo mas interesante: Los p-values
p_values = regr.pvalues
print(regr.pvalues)






#Cuales son mayores a 0.05
print("Variables which p value is > 0.05:")
for index, value in p_values.items():
    if value > 0.05:
        print(index)
		
		
		
		


#Ordenemoslos de mayor a menor
regr.pvalues.sort_values(ascending=False)





<h4>Forward Stepwise Selection</h4>

Partimos de un modelo sin variables predictoras.
Luego comenzamos a agregar de a 1 variable, eligiendo en cada ronda aquel que tenga mayor R2
A medida que vamos conformando nuestros modelos, almacenamos el R2 ajustado
Terminando la iteracion, graficamos los R2 ajustados y seleccionamos el mejor modelo.








#Traemos el dataset de Boston y armaremos nuestro modelo con todas las variables
boston = pd.read_csv('boston.csv')
var_explicativas = boston.drop('MEDV', 1)
var_explicativas = sm.add_constant(var_explicativas)
var_objetivo = boston[['MEDV']]






# Seteamos las condiciones iniciales necesarias
variables = ['const']
iterate_columns = var_explicativas.columns.drop('const',1)
r2_adj = []
vars_size = iterate_columns.size
var_model = {}
# Iteramos k veces, siendo k la cantidad de variables
for k in range(0, vars_size):
    # Acumulamos los valores de R2 para cada variable en los modelos que armaremos
    r2 = {}
    # Iteramos sobre todas las variables disponibles para la ronda
    for var in iterate_columns:
        # Fijamos que variables seran las que definiran nuestro modelo
        var_explicativa = var_explicativas[variables + [var]]
        model = sm.OLS(var_objetivo, var_explicativa)
        regr = model.fit()
        # Almacenamos el R2 para cada set de variables que se prueba
        r2[var] = regr.rsquared_adj
    # Seleccionamos aquella con mayor R2
    var_max_r2 = max(r2.items(), key=operator.itemgetter(1))[0]
    # Almacenamos las variables que describen a ese modelo
    var_model[k] = var_max_r2
    # Almacenamos el valor de R2
    r2_adj.append(r2[var_max_r2])
    # Eliminamos la variable para que deje de estar en consideracion en el ciclo siguiente
    iterate_columns = iterate_columns.drop(var_max_r2,1)
    # Agregamos la variable seleccionada en esta vuelta para que la cuente en el modelo siguiente
    variables.append(var_max_r2)
#Seteamos el valor de la ronda k donde R2 es maximo
r2_max_index = r2_adj.index(max(r2_adj))








def r2_variation(vars_size, r2_adj, title, x_label, y_label):
    """
    Plot the scatter and the curve that shows how R2 value vary over
    every iteration. A title and labels for the graph must be included.
    Also, highlight the point where there is a maximum R2 value. Add information
    about that point. The amount of independent variables is given.
    """
    # Grafiquemos la solucion
    # Los valores para señalar cada iteracion
    x = np.arange(vars_size)
    # El punto donde R2 es maximo
    r2_max_x = r2_adj.index(max(r2_adj))
    r2_max_y = max(r2_adj)
    # Titulo de nuestro grafico y nombre de los ejes
    plt.title(title, loc = 'left')
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    # Grafiquemos la recta con la variacion del R2 en cada vuelta
    plt.plot(x, r2_adj)
    # Grafiquemos dispersion de puntos de X vs R2 ajustado
    plt.scatter(x, r2_adj)
    # Grafiquemos el punto donde R2 es maximo y añadamos su valor en el grafico
    plt.scatter(r2_max_x, r2_max_y, marker='*', s=100, color='red')
    plt.text(r2_max_x * (1 + 0.01), r2_max_y * (0.97)  , round(r2_max_y, 3), fontsize=12)
    plt.text(r2_max_x * (1 + 0.01), r2_max_y * (0.95) , 'k = ' + str(r2_max_x), fontsize=12)
    plt.show()
	
	
	
	
	




r2_variation(vars_size, r2_adj, 'Forward Stepwise Selection', 'k', 'R2')






var_model







# El modelo definitivo
variables = []
i = 0
while i <= r2_max_index:
    variables.append(var_model[i])
    i += 1
var_explicativa = var_explicativas[variables]
var_explicativa = sm.add_constant(var_explicativa)
model = sm.OLS(var_objetivo, var_explicativa)
regr = model.fit()
print(regr.summary())






<h4>Backward Stepwise Selection</h4>

Partimos de un modelo definido por todas variables predictoras.
Luego comenzamos a eliminar de a 1 variable, eligiendo en cada ronda aquel que tenga mayor R2
A medida que vamos conformando nuestros modelos, almacenamos el R2 ajustado
Terminando la iteracion, graficamos los R2 ajustados y seleccionamos el mejor modelo.






boston = pd.read_csv('boston.csv')
var_explicativas = boston.drop('MEDV', 1)
var_explicativas = sm.add_constant(var_explicativas)
var_objetivo = boston[['MEDV']]




# Seteamos las condiciones iniciales necesarias
variables = var_explicativas.columns
iterate_columns = variables.drop('const', 1)
vars_size = iterate_columns.size
r2_adj = []
var_model = {}
# Iteramos k veces, siendo k la cantidad de variables
for k in range(0, vars_size):
    # Acumulamos los valores de R2 para cada variable en los modelos que armaremos
    r2 = {}
    # Indice para insertar las variables temporalmente removidas en su lugar
    i = 1
    # Iteramos sobre todas las variables disponibles para la ronda
    for var in iterate_columns:
        #Eliminamos la variable para probar
        variables = variables.drop(var, 1)
        # Fijamos que variables seran las que definiran nuestro modelo
        var_explicativa = var_explicativas[variables]
        # Insertamos nuevamente la variable que sacamos en el lugar que estaba para la prox ronda
        variables = variables.insert(i, var)
        # Entrenamos el modelo
        model = sm.OLS(var_objetivo, var_explicativa)
        regr = model.fit()
        # Almacenamos el R2 para cada set de variables que se prueba
        r2[var] = regr.rsquared_adj
        # Actualizador de indice
        i += 1
    # Seleccionamos aquella con mayor R2
    var_max_r2 = max(r2.items(), key=operator.itemgetter(1))[0]
    # Almacenamos el valor de R2
    r2_adj.append(r2[var_max_r2])
    # Almacenamos las variables que describen a ese modelo
    var_model[k] = var_max_r2
    # No itera mas sobre esa variable tampoco
    iterate_columns = iterate_columns.drop(var_max_r2, 1)
    # Dejamos de considerar como una variable posible para definir al modelo
    variables = variables.drop(var_max_r2, 1)
#Seteamos el valor de la ronda k donde R2 es maximo
r2_max = r2_adj.index(max(r2_adj))





r2_variation(vars_size, r2_adj, 'Backward Stepwise Selection', 'k', 'R2')



# El modelo definitivo
variables = var_explicativas.columns
i = 0
while i <= r2_max:
    variables = variables.drop(var_model[i], 1)
    i += 1
var_explicativa = var_explicativas[variables]
var_explicativa = sm.add_constant(var_explicativa)
model = sm.OLS(var_objetivo, var_explicativa)
regr = model.fit()
print(regr.summary())







<h4>Backward Stepwise Selection with P-Values</h4>

Partimos de un modelo definido por todas variables predictoras.
Luego eliminamos, en cada vuelta, a la variable con mayor P-Value
A medida que vamos conformando nuestros modelos, almacenamos el R2 ajustado
Terminando la iteracion, graficamos los R2 ajustados y seleccionamos el mejor modelo.





boston = pd.read_csv('boston.csv')
var_explicativas = boston.drop('MEDV', 1)
var_explicativas = sm.add_constant(var_explicativas)
var_objetivo = boston[['MEDV']]






variables = var_explicativas.columns
r2_adj = []
vars_out = []
for k in range(0, vars_size):
    var_explicativa = var_explicativas[variables]
    # Entrenamos el modelo
    model = sm.OLS(var_objetivo, var_explicativa)
    regr = model.fit()
    # Almacenamos el valor de R2
    r2_adj.append(regr.rsquared_adj)
    # Obtenemos los p-values sin considerar la constante
    p_values = regr.pvalues.drop('const')
    print(regr.pvalues)
    pvalue_index = p_values.argmax()
    pvalue_var = p_values.keys()[pvalue_index]
    # Eliminamos la variable cuyo p-value es el mas grande
    variables = variables.drop(pvalue_var)
    # Almacenamos la variable que queda fuera en cada ronda
    vars_out.append(pvalue_var)
	
	
	
	



vars_out





r2_variation(vars_size, r2_adj, 'Backward Stepwise Selection with p-values', 'k', 'R2')







<td></pre>
  <tr>
    <th>Company 3 ML</th>
    
  </tr>
  <tr>
    <th>Company 4 ML</th>
    
  </tr>
  <tr>
    <th>Company 5 ML</th>
    
  </tr>
  <tr>
    <th>Company 6 ML</th>
    
  </tr>
  <tr>
    <th>Company 7 ML</th>
    
  </tr>
  <tr>
    <th>Company 8 ML</th>
    
  </tr>
  <tr>
    <th>Company 9 ML</th>
    
  </tr>
  <tr>
    <th>Company 10 ML</th>
    
  </tr>		 
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
  <tr>
    <td></td>
  </tr>

</table>
    </div>

  <footer>
    <p align="center"><img src="images/logounb.png" alt="h" width="107" height="69"> Alcalde: Javier Antonio Pérez Tel. (503)23792517 Barrio El Centro|  Alcaldia de </a>San Emigdio </a><img src="images/ku.png" alt="t" width="101" height="90"></p>
  </footer>
	
</div>
  <p>&nbsp;</p>
  <!-- javascript at the bottom for fast page loading -->
  <script type="text/javascript" src="js/jquery.js"></script>
  <script type="text/javascript" src="js/jquery.easing-sooper.js"></script>
  <script type="text/javascript" src="js/jquery.sooperfish.js"></script>
  <script type="text/javascript">
    $(document).ready(function() {
      $('ul.sf-menu').sooperfish();
    });
  </script>
</body>
</html>
