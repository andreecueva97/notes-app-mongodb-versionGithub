<!DOCTYPE HTML>
<html>
 
<head>
  <title>DeepLearning</title>
  <meta name="description" content="website description" />
  <meta name="keywords" content="website keywords, website keywords" />
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
  <link rel="stylesheet" type="text/css" href="css/style.css" />
  <!-- modernizr enables HTML5 elements and feature detects -->
  <script type="text/javascript" src="js/modernizr-1.5.min.js"></script>
  <script src="Scripts/AC_RunActiveContent.js" type="text/javascript"></script>
  <style type="text/css">
	  table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td, th {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

tr:nth-child(even) {
  background-color: #dddddd;
}
<!--
.style1 {color: #FFFFFF}
.style2 {color: #6FA5E1}
.style7 {
	color: #0dbbd5;
	font-size: 36px;
}
-->
  </style>
	
</head>
 
<body>
<div id="main">
    <header>
      <div id="logo">
        <div id="logo_text">
          <!-- class="logo_colour", allows you to change the colour of the text -->
          <h1><a href="index.html">Deep Learning de<span class="logo_colour"> </span></a>San Emigdio Especialista Machine learning </h1>
          <h2></h2>
        </div>
      </div>
      <nav>
        <div id="menu_container">
          <ul class="sf-menu" id="nav">
             <li><a href="index.html">Inicio</a></li>
            <li><a href="valores.html">Valores Marginales</a></li>
 
			<li><a href="sanemigdio.html" class="sf-menu" id="nav">San Emigdio</a>
  <ul>
    <li><a href="ubicaciongeografica.html">Ubicacion Geografica</a></li>
    <li><a href="patrimonio.html">Patrimonio Naturales</a></li>
    <li><a href="actividadesculturales.html">Actividades Culturales</a></li>
	<li><a href="turismo.html">Turismo</a></li>
	<li><a href="galeria.html">Galeria</a></li>
  </ul>
</li>
 
 
 
 
			<li><a href="gestionesmunisipales.html" class="sf-menu" id="nav">Gestiones Munisipales</a>
 
			<ul>
    <li><a href="obras.html">Obras</a></li>
    <li><a href="obras sociales.html">Obras Sociales</a></li>
	<li><a href="proyectosmuniscipales.html">Proyectos Munisipales</a></li>
  </ul>
</li>
 
 
            <li><a href="#">Contactenos</a>
              <ul>
                <li><a href="pterm1.html">Correo Institucional</a></li>
                <li><a href="pterm2.html">Facebook.</a></li>
                <li><a href="pterm3.html">Twitter.</a></li>
              </ul>
            </li>
          </ul>
        </div>
      </nav>
    </header>
    <div id="site_content">
      <table width="952" height="656" border="0">
        <tr>
          <th width="236" scope="col"><p class="style2"><img src="images/Imagen1.jpg" width="230" height="270"></p>
          <p class="style2">&nbsp;</p>
          <p class="style2">&nbsp;</p>
          <p class="style2">&nbsp;</p>
          <p class="style2">&nbsp;</p>
          </th>
          <th width="453" height="652" scope="col"><h1 align="center" class="style7">Introduccion al Machine Learning </h1>
            <p align="justify" class="style1"><strong>Queridos Amigos: </strong><br>
            <p align="justify" class="style1">Reciban un afectuoso saludo de  parte de su servidor Javier Antonio Pérez. Para mí es un verdadero honor  presidir al Concejo Municipal del Pueblo de San Emigdio. Departamento de La  Paz, y poder trabajar para todos ustedes de la mano, persiguiendo el desarrollo  local de nuestro querido pueblo. <br>
              Siempre he considerado que  la mejor manera de tener éxito en la vida. Es por medio del dialogo, de la mano  de las personas, de escuchar a los habitantes, de pensar en el bienestar de los  demás, por esta razón mi administración municipal será bajo el mismo enfoque,  un gobierno cercano a la gente. <br>
              Mi persona y Concejo  Municipal estamos comprometidos con todos ustedes por lo que gestionaremos y  llevaremos a cabo obras en las diferentes comunidades de nuestro querido  municipio, las cuales vendrán a mejorar las condiciones de vida de cada uno de  ustedes.<br>
  <br>
              Tenemos la confianza puesta  en Dios quien nos guiará en la mejor toma de decisiones ya que estas van  encaminadas para poder seguir trabajando para el desarrollo integral de nuestro  querido pueblo, para todos los que en el habitamos y todos los que nos visitan,  porque estamos iniciando un verdadero cambio en beneficio de todos y todas.</p>
            <p align="justify" class="style1"><br>
            Que Dios nos bendiga a  todos. </p>
            <p class="style1"><strong>Javier Antonio Pérez.</strong><br>
                        <strong>Alcalde Municipal de San  Emigdio</strong><br>
                        <strong>Depto. La Paz.   </strong></p>
            <p class="style1">&nbsp;</p>
            <p class="style1">&nbsp;</p>          </th>
          <th width="249" scope="col"><p align="right"><img src="images/ku.png" alt="k" width="243" height="267"></p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
            <p align="center">&nbsp;</p>
          </th>
        </tr>
      </table>
      <div align="justify"></div>
	  <table width="2000">
  <tr>
    <th>Company 0 ML</th>
    
  </tr>
  <tr>
	  
	  
	  
	  
	  <td><pre>
	matsplot.py
	import numpy as np
"""
Completar las funciones señaladas con la logica
correspondiente segun conceptos vistos en clase 0. 
No remover ni modifcar la constante X
"""
X = np.arange(10)

def y_values(x, mode):
    """
    Given an array of X values, return the y
    values according to the mode parameter.
    mode can be l for a linear function y = m*x + b
    where b = 2 and m = 3 and q for a quadratic function
    y = x^2
    >>> y_values(X, 'l')
    array([ 2,  5,  8, 11, 14, 17, 20, 23, 26, 29])
    >>> y_values(X, 'q')
    array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])
    """
    pass
	
	def plot_x_y(x,y, mode):
    """
    Given x and y values of the same size, use matplot 'plt'
    to plot a specific chart. This function can be solve in one line
    """
    pass
	
	if __name__ == '__main__':
    #a sample result, try as many as you wany
    Y = y_values(X, 'l')
    plot_x_y(X, Y, 'r')
    plt.show()
</pre></td>

  
  </tr>
  <tr>
    <td><pre>
	panda.py
	import pandas as pd
"""
Completar las funciones señaladas con la logica
correspondiente segun conceptos vistos en clase 0. 
No modificar la constante SRC. Todas las funciones
se resuelven en una o dos lineas de codigo
"""
SRC = {"Name":
           ["Geoffrey Hinton",
            "Michael I Jordan",
            "Andrew Ng",
            "Yann LeCun",
            "Yoshua Bengio"],
       "Age": [73, 65, 45, 61, 57],
       "Country":
           ["UK", "US", "UK", "FR", "FR"]}


def set_dataframe(source):
    """
    From the SRC constant, create a panda data frame
    and return it. This will be a useful helper function
    """
    pass


def average_age(data_frame):
    """
    Given a data frame, get the average
    age of all of the names in list
    >>> average_age( set_dataframe(SRC) )
    60.2
    """
    pass


def people_from(a_country, data_frame):
    """
    Given a data frame, get all the people
    from a given country
    """
    pass

if __name__ == '__main__':
    data = set_dataframe(SRC)
    print(data)
    print("Average Age:", average_age(data))
    print(people_from("UK", data))
	</pre></td>
   
  </tr>
  <tr>
    <td><pre>
	warm-up.py
"""
Completar las siguientes funciones definidas de modo que cumplan
con la logica solicitada. El proposito de este ejercicio es recordar
elementos de Python (req. Informatica General). Se asignan tests previamente
definidos (no es necesario agregar nuevos casos).
"""


def list_has_even_size(lst):
    """
    Given a list return True if its size
    is an even number, otherwise False

    >>> list_has_even_size([1, 2, 3])
    False
    >>> list_has_even_size(['a', 'b', 'c', 'd'])
    True
    >>> list_has_even_size([])
    True
    """
    pass


def sum_of_elements(lst):
    """
    Given a list with integer numbers
    return the sum of all the elements.
    >>> sum_of_elements([1, 25, 45, 30])
    101
    >>> sum_of_elements([5, 3, -1])
    7
    >>> sum_of_elements([])
    0
    >>> sum_of_elements([-5, -2])
    -7
    """
    pass


def remove_elements(array):
    """
    Given a 2d array, return an array
    that contains only the rows that have not
    None as an element
    >>> remove_elements([[1, 2], [3, 4]])
    [[1, 2], [3, 4]]
    >>> remove_elements([[1, None], [2, 3]])
    [[2, 3]]
    >>> remove_elements([[1, None], [1, None]])
    []
    """
    pass


def replace_value(array):
    """
    Given a 2d array, replace every 'x'/'X' character
    with an 'o'/'O' character.
    >>> replace_value([['a', 'x'], ['o', 'b']])
    [['a', 'o'], ['o', 'b']]
    >>> replace_value([['a', 'b'], ['c', 'd']])
    [['a', 'b'], ['c', 'd']]
    >>> replace_value([['X', 'x'], ['xx', 'XX']])
    [['O', 'o'], ['xx', 'XX']]
    """
    pass
		  </pre></td>
   
  </tr>
  
  <tr>
    <td></td>
    
  </tr>
<tr>
    <th>Company 1 ML</th>
		  </tr>
    <td><pre>

1           regresion lineal


<h4>Regresion Lineal Simple</h4>

Vamos a empezar con lo que verdaderamente nos trae a esta practica: Modelos de Regresion.

Comenzaremos con un modelo de Regresion Lineal Simple. Consideraciones:
- Trabajaremos con un Dataset que incluye datos de las propiedades de Boston
- La libreria a usar es scikit-learn (https://jmlr.csail.mit.edu/papers/volume12/pedregosa11a/pedregosa11a.pdf)
- Usaremos matplot para visualizar la data.



#Importamos la libreria sklearn, una libreria sobre machine learning en Python. 
#De estas librerias traemos datasets,que son datos que vienen con la libreria 
#para fines educativos. Traeremos el modelo de regresion lineal que vamos a entrenar.
from sklearn import datasets, linear_model
#De la libreria sklearn.metrics traeremos aqullas formulas que nos permiten 
#evaluar la precision de las predicciones
from sklearn.metrics import mean_squared_error, r2_score



Nuestro dataset sera un conjunto de datos sobre viviendas en Boston. (https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset)
Una cosa a tener en cuenta: sklearn viene con los datasets de Boston cargados y su manipulacion para entrenar el modelo es trivial. Pero nosotros no vamos a hacer las cosas tan a alto nivel. Vamos a usar lo aprendido anteriormente para que manipulen los datos desde 0. Por ellos vamos a construir nosotros nuestro DF con panda



#Antes de comenzar, recomiendo leer las referencias del dataset
import pandas as pd
boston = pd.read_csv('boston.csv')
#Veamos los primeros elementos
boston.head()
#Nuestro dataset tiene 13 variables explicativas (CRIM, ZN, INDUS, CHAS, ..., LSTAT)
#La variable objetivo es MEDV y significa Median value of owner-occupied homes in $1000’s




var_explicativas = boston[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]
var_explicativas.head()


var_objetivo = boston[['MEDV']]
var_objetivo.head()



import matplotlib.pyplot as plt

loop

variables = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
for var in variables:
    x = boston[[var]]
    plt.scatter(x, var_objetivo)
    plt.xlabel(var)
    plt.ylabel("MEDV")
    plt.show()
	
	
	Entrenemos un modelo de regresion lineal simple. Nuestro objetivo es predecir el precio de una vivienda en funcion de LSTAT.
	
	
	#Paso 1: Cargamos el modelo
regr = linear_model.LinearRegression()



#La variable explicativa sera la columna Rooms
var_explicativa = boston[['RM']]
#Paso 2: (es clave) entrenar el modelo en base a los datos fuente
regr.fit(var_explicativa, var_objetivo)

pendiente positiva tiene sentido??

#Veamos cual es el beta 1 de este modelo. (la pendiente)
beta_1 = regr.coef_
beta_1

variable independiente negatvia tiene sentido??


beta_0 = regr.intercept_
beta_0


	prediccion_precios = regr.predict(var_explicativa)
prediccion_precios


#Grafiquemos la linea de regresion y la dispersion de datos
plt.scatter(var_explicativa, var_objetivo)
plt.plot(var_explicativa, prediccion_precios, 'r')
plt.xlabel('LSTAT')
plt.ylabel('MEDV')



import numpy as np
precio = np.transpose(var_objetivo)

for index, row in var_objetivo.iterrows():
    print(index, row)
	
	
	
	
	#Calculemos el error cuadrado medio
sce = 0
for i in range(0, precio.size):
    sce += np.power(precio[i] - prediccion_precios[i], 2)
mse = sce / precio.size
mse



#Cuanto es el MSE o Error Cuadrado Medio
mean_squared_error(var_objetivo, prediccion_precios)




#Cuanto es el R2
r2_score(var_objetivo, prediccion_precios)



<h4>Regresion Lineal Multiple</h4>

Ahora entrenaremos un modelo de regresion que se explique por dos variables: LSTAT y RM. Para ello utilizaremos tambien regresion lineal



#En este caso elegiremos dos columnas como variables explicativas
var_explicativa = boston[['LSTAT', 'RM', 'NOX']]
var_explicativa.head()



ax = plt.figure().add_subplot(projection='3d')
ax.scatter(boston['LSTAT'], boston['RM'], boston['MEDV'])
ax.set_xlabel('LSTAT')
ax.set_ylabel('ROOMS')
ax.set_zlabel('MEDV')


#Entrenamos el modelo, la var_objetivo sigue siendo MEDV (declarada previamente)
regr.fit(var_explicativa, var_objetivo)




#Veamos cuales son los coeficientes Beta 1 y Beta 2
regr.coef_



#Y ahora cual es Beta 0
regr.intercept_




#Veamos que resultados da el modelo
prediccion_prec = regr.predict(var_explicativa)
prediccion_prec



#Calculemos el error cuadrado medio
sce = 0
for i in range(0, precio.size):
    sce += np.power(precio[i] - prediccion_precios[i], 2)
mse = sce / precio.size
mse



mean_squared_error(var_objetivo, prediccion_precios)



#Cuanto es el R2
r2_score(var_objetivo, prediccion_precios)




<h4>Calculemos $R^{2}$ Ajustado</h4>

Segun lo visto en clase, definimos a $R^{2}$ como:

$R^{2} = 1 - \frac{RSS}{TSS}$

donde RSS es Residual Sum of Squares y se calcula como:

$RSS = \Sigma(y_{i} - Y_{i})^{2}$

y TSS:

$TSS = \Sigma(y_{i} - y_{med-i})^{2}$

Para $R^{2}_{ajus}$ queremos penalizar la incorporacion de nuevas variables:

$R^{2}_{ajus} = 1 - \frac{RSS / (n - d -1)}{TSS / (n-1)}$

remplazando terminos:

$R^{2}_{ajus} = 1 - (1 - R^{2})(\frac{n-1}{n-d-1})$






#En base a lo definido anteriormente calculemos R2 ajustado
#N es el numero de ocurrencias
n = var_objetivo.size
#d es el numero de variables explicativas
d = var_explicativas.columns.size
print('n: ', n, 'd: ', d)




#Planteamos nuestra formula y veamos el resultado
r2_ajust = 1 - (1 - r2) * ((n - 1) / (n - d - 1))
print('R2 Ajustado:', round(r2_ajust, 3))
print('R2: ', round(r2, 3))



</pre></td>
  
 		
  <tr>
    <th>Company 2 ML</th>
    
  </tr>
practica 2
<td><pre>
<h4>técnicas de selección discutidas la última clase: Forward Stepwise Selection, Backward Stepwise Selection y Backward Stepwise Selection with p-values.
</h4>


#Importamos las librerias que vamos a usar hoy
import pandas as pd
import numpy as np
from sklearn import linear_model
from sklearn.metrics import r2_score
import statsmodels.api as sm
import operator
import matplotlib.pyplot as plt





#Traemos el dataset de Boston y armaremos nuestro modelo con todas las variables
boston = pd.read_csv('boston.csv')
var_explicativas = boston.drop('MEDV', 1)
var_objetivo = boston[['MEDV']]





<h4>Una nueva libreria: StatsModels</h4>

Esta libreria permite hacer estimaciones para diferentes modelos estadísticos, además de permitir hacer pruebas estadísticas y exploración de data https://www.statsmodels.org/stable/index.html.

Respecto a SKLearn, para Regresión Lineal las diferencias no son significativas. Podemos leer este artículo donde se comparan ambos módulos: https://becominghuman.ai/stats-models-vs-sklearn-for-linear-regression-f19df95ad99b 

¿Por que lo utilizaremos? porque nos brinda muchos detalles estadisticos que SKLearn no.







#En StatsModels debemos agregar nosotros mismo la constante para el armado de nuestro modelo.
var_explicativas = sm.add_constant(var_explicativas)
var_explicativas.head()





#Definimos nuestro modelo, OLS significa Ordinary Least Squares. Esto indica la forma en la que
#se calcularan los parametros
model = sm.OLS(var_objetivo, var_explicativas)





#Entrenamos el modelo (notemos las diferencias con sklearn)
regr = model.fit()





#Veamos todos los datos que nos entrega StatsModels
print(regr.summary())





#Para acceder al R2 y R2 Ajustado
r2 = regr.rsquared
r2_adj = regr.rsquared_adj
print('R2: ', round(r2, 3))
print('R2 Ajustado:', round(r2_adj, 3))







#Y lo mas interesante: Los p-values
p_values = regr.pvalues
print(regr.pvalues)






#Cuales son mayores a 0.05
print("Variables which p value is > 0.05:")
for index, value in p_values.items():
    if value > 0.05:
        print(index)
		
		
		
		


#Ordenemoslos de mayor a menor
regr.pvalues.sort_values(ascending=False)





<h4>Forward Stepwise Selection</h4>

Partimos de un modelo sin variables predictoras.
Luego comenzamos a agregar de a 1 variable, eligiendo en cada ronda aquel que tenga mayor R2
A medida que vamos conformando nuestros modelos, almacenamos el R2 ajustado
Terminando la iteracion, graficamos los R2 ajustados y seleccionamos el mejor modelo.








#Traemos el dataset de Boston y armaremos nuestro modelo con todas las variables
boston = pd.read_csv('boston.csv')
var_explicativas = boston.drop('MEDV', 1)
var_explicativas = sm.add_constant(var_explicativas)
var_objetivo = boston[['MEDV']]






# Seteamos las condiciones iniciales necesarias
variables = ['const']
iterate_columns = var_explicativas.columns.drop('const',1)
r2_adj = []
vars_size = iterate_columns.size
var_model = {}
# Iteramos k veces, siendo k la cantidad de variables
for k in range(0, vars_size):
    # Acumulamos los valores de R2 para cada variable en los modelos que armaremos
    r2 = {}
    # Iteramos sobre todas las variables disponibles para la ronda
    for var in iterate_columns:
        # Fijamos que variables seran las que definiran nuestro modelo
        var_explicativa = var_explicativas[variables + [var]]
        model = sm.OLS(var_objetivo, var_explicativa)
        regr = model.fit()
        # Almacenamos el R2 para cada set de variables que se prueba
        r2[var] = regr.rsquared_adj
    # Seleccionamos aquella con mayor R2
    var_max_r2 = max(r2.items(), key=operator.itemgetter(1))[0]
    # Almacenamos las variables que describen a ese modelo
    var_model[k] = var_max_r2
    # Almacenamos el valor de R2
    r2_adj.append(r2[var_max_r2])
    # Eliminamos la variable para que deje de estar en consideracion en el ciclo siguiente
    iterate_columns = iterate_columns.drop(var_max_r2,1)
    # Agregamos la variable seleccionada en esta vuelta para que la cuente en el modelo siguiente
    variables.append(var_max_r2)
#Seteamos el valor de la ronda k donde R2 es maximo
r2_max_index = r2_adj.index(max(r2_adj))








def r2_variation(vars_size, r2_adj, title, x_label, y_label):
    """
    Plot the scatter and the curve that shows how R2 value vary over
    every iteration. A title and labels for the graph must be included.
    Also, highlight the point where there is a maximum R2 value. Add information
    about that point. The amount of independent variables is given.
    """
    # Grafiquemos la solucion
    # Los valores para señalar cada iteracion
    x = np.arange(vars_size)
    # El punto donde R2 es maximo
    r2_max_x = r2_adj.index(max(r2_adj))
    r2_max_y = max(r2_adj)
    # Titulo de nuestro grafico y nombre de los ejes
    plt.title(title, loc = 'left')
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    # Grafiquemos la recta con la variacion del R2 en cada vuelta
    plt.plot(x, r2_adj)
    # Grafiquemos dispersion de puntos de X vs R2 ajustado
    plt.scatter(x, r2_adj)
    # Grafiquemos el punto donde R2 es maximo y añadamos su valor en el grafico
    plt.scatter(r2_max_x, r2_max_y, marker='*', s=100, color='red')
    plt.text(r2_max_x * (1 + 0.01), r2_max_y * (0.97)  , round(r2_max_y, 3), fontsize=12)
    plt.text(r2_max_x * (1 + 0.01), r2_max_y * (0.95) , 'k = ' + str(r2_max_x), fontsize=12)
    plt.show()
	
	
	
	
	




r2_variation(vars_size, r2_adj, 'Forward Stepwise Selection', 'k', 'R2')






var_model







# El modelo definitivo
variables = []
i = 0
while i <= r2_max_index:
    variables.append(var_model[i])
    i += 1
var_explicativa = var_explicativas[variables]
var_explicativa = sm.add_constant(var_explicativa)
model = sm.OLS(var_objetivo, var_explicativa)
regr = model.fit()
print(regr.summary())






<h4>Backward Stepwise Selection</h4>

Partimos de un modelo definido por todas variables predictoras.
Luego comenzamos a eliminar de a 1 variable, eligiendo en cada ronda aquel que tenga mayor R2
A medida que vamos conformando nuestros modelos, almacenamos el R2 ajustado
Terminando la iteracion, graficamos los R2 ajustados y seleccionamos el mejor modelo.






boston = pd.read_csv('boston.csv')
var_explicativas = boston.drop('MEDV', 1)
var_explicativas = sm.add_constant(var_explicativas)
var_objetivo = boston[['MEDV']]




# Seteamos las condiciones iniciales necesarias
variables = var_explicativas.columns
iterate_columns = variables.drop('const', 1)
vars_size = iterate_columns.size
r2_adj = []
var_model = {}
# Iteramos k veces, siendo k la cantidad de variables
for k in range(0, vars_size):
    # Acumulamos los valores de R2 para cada variable en los modelos que armaremos
    r2 = {}
    # Indice para insertar las variables temporalmente removidas en su lugar
    i = 1
    # Iteramos sobre todas las variables disponibles para la ronda
    for var in iterate_columns:
        #Eliminamos la variable para probar
        variables = variables.drop(var, 1)
        # Fijamos que variables seran las que definiran nuestro modelo
        var_explicativa = var_explicativas[variables]
        # Insertamos nuevamente la variable que sacamos en el lugar que estaba para la prox ronda
        variables = variables.insert(i, var)
        # Entrenamos el modelo
        model = sm.OLS(var_objetivo, var_explicativa)
        regr = model.fit()
        # Almacenamos el R2 para cada set de variables que se prueba
        r2[var] = regr.rsquared_adj
        # Actualizador de indice
        i += 1
    # Seleccionamos aquella con mayor R2
    var_max_r2 = max(r2.items(), key=operator.itemgetter(1))[0]
    # Almacenamos el valor de R2
    r2_adj.append(r2[var_max_r2])
    # Almacenamos las variables que describen a ese modelo
    var_model[k] = var_max_r2
    # No itera mas sobre esa variable tampoco
    iterate_columns = iterate_columns.drop(var_max_r2, 1)
    # Dejamos de considerar como una variable posible para definir al modelo
    variables = variables.drop(var_max_r2, 1)
#Seteamos el valor de la ronda k donde R2 es maximo
r2_max = r2_adj.index(max(r2_adj))





r2_variation(vars_size, r2_adj, 'Backward Stepwise Selection', 'k', 'R2')



# El modelo definitivo
variables = var_explicativas.columns
i = 0
while i <= r2_max:
    variables = variables.drop(var_model[i], 1)
    i += 1
var_explicativa = var_explicativas[variables]
var_explicativa = sm.add_constant(var_explicativa)
model = sm.OLS(var_objetivo, var_explicativa)
regr = model.fit()
print(regr.summary())







<h4>Backward Stepwise Selection with P-Values</h4>

Partimos de un modelo definido por todas variables predictoras.
Luego eliminamos, en cada vuelta, a la variable con mayor P-Value
A medida que vamos conformando nuestros modelos, almacenamos el R2 ajustado
Terminando la iteracion, graficamos los R2 ajustados y seleccionamos el mejor modelo.





boston = pd.read_csv('boston.csv')
var_explicativas = boston.drop('MEDV', 1)
var_explicativas = sm.add_constant(var_explicativas)
var_objetivo = boston[['MEDV']]






variables = var_explicativas.columns
r2_adj = []
vars_out = []
for k in range(0, vars_size):
    var_explicativa = var_explicativas[variables]
    # Entrenamos el modelo
    model = sm.OLS(var_objetivo, var_explicativa)
    regr = model.fit()
    # Almacenamos el valor de R2
    r2_adj.append(regr.rsquared_adj)
    # Obtenemos los p-values sin considerar la constante
    p_values = regr.pvalues.drop('const')
    print(regr.pvalues)
    pvalue_index = p_values.argmax()
    pvalue_var = p_values.keys()[pvalue_index]
    # Eliminamos la variable cuyo p-value es el mas grande
    variables = variables.drop(pvalue_var)
    # Almacenamos la variable que queda fuera en cada ronda
    vars_out.append(pvalue_var)
	
	
	
	



vars_out





r2_variation(vars_size, r2_adj, 'Backward Stepwise Selection with p-values', 'k', 'R2')







</pre></td>
  <tr>
    <th>Company 3 ML</th>
    
  </tr>
practica 3
<td><pre>

<h4>Regresion No Lineal: Polinómica y Logarítmica.</h4>

valernos de ajustes lineales para tendencias no lineales. Ademas aplicaremos ANOVA para la seleccion de variables






<h5>Regresion No Lineal: Polinomial</h5>
    Veamos como mejorar nuestro R2 del modelo de Regresion Lineal de la practica 1
	





import pandas as pd
import statsmodels.api as sm
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt
import numpy as np











#Traemos el dataset de Boston y armaremos nuestro modelo con todas las variables
boston = pd.read_csv('boston.csv')
var_explicativas = boston[['LSTAT']]
var_objetivo = boston[['MEDV']]





















var_explicativas

















#Creamos los valores del polinomio a partir de X
polynomial_features = PolynomialFeatures(degree=2)
xp = polynomial_features.fit_transform(var_explicativas)
#Devuelve X^0, X^1, X^2
print(xp)
















#Creamos y entrenamos a nuestro modelo.
model = sm.OLS(var_objetivo, xp)
regr = model.fit()
print(regr.summary())













#Predizcamos los valores de precio
precios = regr.predict(xp)









plt.scatter(var_explicativas, var_objetivo)
plt.scatter(var_explicativas, precios, color='red')
plt.show()













# Comparemos como cambia a medida que aumenta el grado del polinomio
for i in range(1, 10):
    # Obtengo los valores del polinomio a partir de x
    polynomial_features=PolynomialFeatures(degree=i)
    xp = polynomial_features.fit_transform(var_explicativas)
    # Entreno el modelo
    model = sm.OLS(var_objetivo, xp)
    regr = model.fit()
    # Grafico la dispersion de puntos
    plt.scatter(var_explicativas, var_objetivo)
    # Creo una columna con valores X para realizar la prediccion
    x = np.arange(30, step=0.1)
    x = x[:,None]
    xp = polynomial_features.fit_transform(x)
    # Obtengo los valores y a a partir de los x 
    y = regr.predict(xp)
    # Grafico x vs y, para ver la grafica del polinomio
    plt.plot(x, y, color='red')
    # Añado valores
    plt.xlabel('LSTAT')
    plt.ylabel('MEDV')
    plt.title('Polinomio de grado ' + str(i))
    plt.text(30, 40, 'R2 adj: ' + str(round(regr.rsquared_adj,3)))
    plt.show()
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	


# El que mejor ajusta en este caso es un polinomio de grado 3
polynomial_features=PolynomialFeatures(degree=3)
xp = polynomial_features.fit_transform(var_explicativas)
# Entreno el modelo
model = sm.OLS(var_objetivo, xp)
regr = model.fit()
print(regr.summary())







<h5>Regresio No Lineal: Logaritmica</h5>







Prueben con esta herramienta https://miniwebtool.com/es/log-base-10-calculator/ que se calcula apropiadamente 








# Seteamos las variables y las observemos
x_log = np.log10(var_explicativas)
y_log = np.log10(var_objetivo)
plt.scatter(x_log, y_log)
plt.xlabel('Log(LSTAT)')
plt.ylabel('Log(MEDV)')
plt.show()










# Entrenamos el modelo
x_log = sm.add_constant(x_log)
model = sm.OLS(y_log, x_log)
regr = model.fit()
print(regr.summary())










# Grafiquemos nuestro modelo
y_log_pred = regr.predict(x_log)
plt.scatter(x_log['LSTAT'], y_log)
plt.plot(x_log['LSTAT'], y_log_pred, 'r')
plt.xlabel('Log(LSTAT)')
plt.ylabel('Log(MEDV)')
plt.text(1.3, 1.6, 'R2 ADJ: ' + str(round(regr.rsquared_adj,3)))
plt.show()











# Encontramos los beta
beta_0 = regr.params[0]
beta_1 = regr.params[1]
# Transformamos los valores
a = np.power(10, beta_0)
# Armamos los valores del modelo
x = np.arange(start=2, stop=37, step=0.1)
x = x[:,None]
y_model = a * np.power(x, beta_1)
print(x.size, y_model.size)












# Visualicemos
plt.scatter(var_explicativas, var_objetivo)
plt.plot(x, y_model, 'r')
plt.xlabel('LSTAT')
plt.ylabel('MEDV')
plt.text(30, 40, 'R2 ADJ: ' + str(round(regr.rsquared_adj, 3)))
plt.show()











</pre></td>
  <tr>
    <th>Company 4 ML</th>
    
  </tr>
practica 4 
<td><pre>

<h4> Regresion Logística y kNN.</h4>





import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn import neighbors






# Cargamos la data
df = pd.read_csv('Churn_Modelling.csv')
df.head()






# Notemos que hay variables que no aportan al modelo como RowNumber, CustomerId y Surname
df = df.drop(['RowNumber', 'CustomerId', 'Surname', 'Geography'], axis=1)
df.head()





# Tenemos variables que debemos cambiar su forma de representacion como Geography y Gender
# Creamos los valores para geography
#geography = pd.factorize(df['Geography'])
#geography







# Obtenemos el indice donde se encontraba Geogrpahy
index = df.columns.get_loc('Geography')
# Eliminamos la columna
df = df.drop('Geography', axis=1)
# Insertamos la nueva
df.insert(index, 'Geography', geography[0])
df.head()







# Creamos los valores para Gender
gender = pd.factorize(df['Gender'])
# Obtenemos el indice donde se encontraba Gender
index = df.columns.get_loc('Gender')
# Eliminamos la columna
df = df.drop('Gender', axis=1)
# Insertamos la nueva
df.insert(index, 'Gender', gender[0])
df.head()








#Veamos como influye el credit score y la edad en la decision de abandonar el producto
x = df['Age']
y = df['CreditScore']
e = df['Exited']
plt.scatter(x,y,c=e)
plt.colorbar()
plt.show()








#Veamos como influye la edad y la cantidad de productos en la decision de abandonar el producto
x = df['NumOfProducts']
y = df['Age']
e = df['Exited']
plt.scatter(x,y,c=e)
plt.colorbar()
plt.show()







<h5>Regresión Logística</h5>

Vamos a usar la libreria SKLearn para generar y entrenar nuestro modelo. Luego graficaremos el espacio de soluciones








y = df['Exited']
x = df.drop('Exited', axis=1)
rlog = LogisticRegression().fit(x, y)
# Classes nos devuelve que tipos de etiquetas usa el clasificador
rlog.classes_








# Veamos los coeficientes 
rlog.coef_






# Prediccion para cada input
rlog.predict_proba(x)




# Y si queremos cambiar el threshold? Default =0.5
x





# Calculemos el error
y = df['Exited']
x = df.drop('Exited', axis=1)
difference = y - rlog.predict(x)
count = 0
for i in range(0, difference.size):
    # aquellas que calculo bien
    if difference[i] == 0:
        count +=1
accuracy = count/difference.size
accuracy






# Aqui calculado por ela libreria
rlog.score(x, y)







# Grafiquemos nuestra solucion
x_values = df['NumOfProducts']
y = df['Age']
e = rlog.predict(x)
plt.scatter(x_value,y,c=e)
plt.colorbar()

plt.xlabel('NumOfProducts')
plt.ylabel('Age')
plt.show()





#Veamos como influye el credit score y la edad en la decision de abandonar el producto
X = df['Age']
y = df['CreditScore']
e = rlog.predict(x)
plt.scatter(X,y,c=e)
plt.colorbar()
plt.xlabel('Age')
plt.ylabel('CreditScore')
plt.show()








#Veamos como influye el credit score y la edad en la decision de abandonar el producto
x_value = df['Tenure']
y = df['CreditScore']
e = rlog.predict(x)
plt.scatter(x_value,y,c=e)
plt.colorbar()
plt.xlabel('Tenure')
plt.ylabel('CreditScore')
plt.show()






<h5>K-Nearest Neighbours</h5>







y = df['Exited']
x = df.drop('Exited', axis=1)
k = 5
# Existe el modo Uniform (cada distancia tiene igual importancia) o Distance (mas peso a los mas
# cercanos)
knn = neighbors.KNeighborsClassifier(k, weights='uniform').fit(x, y)






knn.get_params()








knn.score(x,y)









#Veamos como influye el credit score y la edad en la decision de abandonar el producto
X = df['Age']
y = df['CreditScore']
e = knn.predict(x)
plt.scatter(X,y,c=e)
plt.colorbar()
plt.xlabel('Age')
plt.ylabel('CreditScore')
plt.show()








</pre></td>

  <tr>
    <th>Company 5 ML</th>
    
  </tr>
	
<td><pre>


<h4>Cross Validation</h4>


 armado de Datasets de Test/Train y 
exploraremos las tecnicas de Cross Validation










import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split









#Traemos el dataset de Boston y armaremos nuestro modelo con todas las variables
boston = pd.read_csv('boston.csv')
boston.head()










X = boston['LSTAT']
y = boston['MEDV']
X







# Forma 1 de hacerlo
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Forma 2 de hacerlo
'''
df = boston[['LSTAT', 'MEDV']]
train = df.sample(frac=0.8,random_state=200) #random state is a seed value
test = df.drop(train.index)
X_train = train['LSTAT']
y_train = train['MEDV']
X_test = test['LSTAT']
y_test = test['MEDV']
'''







print('Size of dataset:',len(boston),'| Size of Train:',X_train.size,'| Size of Test:',X_test.size)







#print('Size of dataset:',len(boston),'| Size of Train:',len(train),'| Size of Test:',len(test))








print("Train: ", X_train,"\nTest: ", X_test)










# armamos los dataset de training y test en proporcion 80-20
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)














<h5>K-Fold Cross Validation</h5>

<p>Modelo 1 : Y = B1 * X1</p>

<p>Modelo 2 : Y = B2 * X2</p>

<p>Modelo 3 : Y = B1 * X1 + B2 * X2</p>














mse = {}






<h2>MODELO 1 k fold cross validation</h2>






import sklearn
from sklearn import linear_model
from sklearn.model_selection import cross_val_score
boston = pd.read_csv('boston.csv')







X = boston[['LSTAT']]
y = boston[['MEDV']]








model_1 = linear_model.LinearRegression()
scores = cross_val_score(model_1, X, y, cv=5, scoring='neg_mean_squared_error')
abs(scores)








¿ Por que neg MSE ? https://stackoverflow.com/questions/48244219/is-sklearn-metrics-mean-squared-error-the-larger-the-better-negated






mse['model1'] = abs(scores.mean())







<h2>MODELO 2 k fold cross validation</h2>






X = boston[['RM']]
y = boston[['MEDV']]








X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)









model_2 = linear_model.LinearRegression().fit(X_train, y_train)
scores = cross_val_score(model_2, X, y, cv=5, scoring='neg_mean_squared_error')
abs(scores)








mse['model2'] = abs(scores.mean())










<h2>MODELO 3 k fold cross validation</h2>








X = boston[['RM', 'LSTAT']]
y = boston[['MEDV']]






X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)







model_3 = linear_model.LinearRegression().fit(X_train, y_train)
scores = cross_val_score(model_3, X, y, cv=5, scoring='neg_mean_squared_error')
abs(scores)






mse['model3'] = abs(scores.mean())




mse




</pre></td>
  <tr>
    <th>Company 6 ML</th>
    
  </tr>
practica 6
<td><pre>


import pandas as pd





# PREPARAMOS LA DATA
df = pd.read_csv('Churn_Modelling.csv')
df = df.drop(['RowNumber', 'CustomerId', 'Surname', ], axis=1)
geography = pd.factorize(df['Geography'])
index = df.columns.get_loc('Geography')
# Eliminamos la columna
df = df.drop('Geography', axis=1)
# Insertamos la nueva
df.insert(index, 'Geography', geography[0])
gender = pd.factorize(df['Gender'])
# Obtenemos el indice donde se encontraba Gender
index = df.columns.get_loc('Gender')
# Eliminamos la columna
df = df.drop('Gender', axis=1)
# Insertamos la nueva
df.insert(index, 'Gender', gender[0])
df.to_csv('churn_cleaned.csv', index=False)












# Entrenamos nuestro modelo
from sklearn.linear_model import LogisticRegression
X = df.drop(['Exited'], axis=1)
y = df['Exited']
model = LogisticRegression().fit(X,y)












# Clasificamos con un umbral a determinar
y_prob = model.predict_proba(X)
y_pred = []
threshold = 0.75
for pro in y_prob:
    if pro[1] > threshold:
        y_pred.append(1)
    else:
        y_pred.append(0)
		
		
		
		
		
		
		
		
		









# Utilizamos la funcion que nos retorna la matriz de confusion
from sklearn.metrics import confusion_matrix
matrix = confusion_matrix(y, y_pred)

tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()







matrix










print(tn)
print(fp)
print(fn)
print(tp)









print("       1           0")
print("1    ",tp,"          ",fp)
print("0    ",fn,"       ",tn)









X = df[['Balance', 'HasCrCard']]
y = df['Exited']










from sklearn import tree
clf = tree.DecisionTreeClassifier().fit(X,y)






clf.score(X,y)








pred_proba = clf.predict_proba(X)
pred_proba






y_pred = clf.predict(X)
y_pred







import matplotlib.pyplot as plt
plt.figure(figsize=(12,12))
tree.plot_tree(clf, fontsize=10)
plt.show()







y = clf.predict([[152000,0]])
y








</pre></td>
  <tr>
    <th>Company 7 ML</th>
    
  </tr>
practica 7
<td><pre>


<h4>BAGGING: Boostrap Aggregation</h4>








# librerias a utilizar
import pandas as pd
from sklearn import tree








df = pd.read_csv('churn_cleaned.csv')
df








def boostrap_sample(df, n, p):
    boostrap = []
    for b in range(0, n):
        # replace True permite que haya items que se repitan en el df
        boostrap.append(df.sample(frac=p, replace=True))
    return boostrap
	
	
	
	
	
	
	

b_sample = boostrap_sample(df, 3, 0.5)
b_sample










def voting(lst):
    one = lst.count(1)
    zero = lst.count(0)
    if one > zero:
        return 1
    return 0
	
	
	
	
	



def bagging_predict(b_sample, x, target):
    # almacenar predicciones
    pred = []
    for b in range(0, len(b_sample)):
        # create X and y value
        X = b_sample[b].drop([target], axis=1)
        y = b_sample[b][target]
        # train decision tree
        clf = tree.DecisionTreeClassifier().fit(X,y)
        # append the prediction in the array
        pred.append(clf.predict(x)[0])
    return voting(pred)
	
	
	
	
	
	





def count_hits(values, df, target):
    count = 0
    for key in values:
        values[key] = voting(values[key])
        if key in df[target].index.values and values[key] == df[target][key]:
            count += 1
    return count
	
	
	
	



def bagging_score(b_sample, df, target):
    # almacenar predicciones
    pred = []
    dfs = {}
    for b in range(0, len(b_sample)):
        df_test = df.drop(b_sample[b].index.values)
        # create X and y value
        X = b_sample[b].drop([target], axis=1)
        y = b_sample[b][target]
        # create X test
        X_test = df_test.drop([target], axis=1)
        # train decision tree
        clf = tree.DecisionTreeClassifier().fit(X,y)
        pred = clf.predict(X_test)
        # iterar sobre las predicciones y agregarlas al diccionario
        # si ya esta el indice, concatenar
        # si no, agregar
        for i in range(0, len(pred)):
            if X_test.index.values[i] in dfs:
                dfs[X_test.index.values[i]].append(pred[i])
            else:
                dfs[X_test.index.values[i]] = [pred[i]]
    #debemos aplicar voting sobre cada key del arreglo
    score = count_hits(dfs, df, target)
    return score / len(dfs)
	
	







# create 100 boostrap samples with 50% original size each one
b_sample = boostrap_sample(df, 3, 0.5)








x = [[500, 0, 1, 40, 1, 5000, 1, 1, 0, 100000]]
# realizaremos 100 muestras boostrap y cada muestra sera del 50% del tamaño del df original
pred = bagging_predict(b_sample, x, 'Exited')
print("CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, isActiveMember, EstimatedSalary")
print(x[0])
if pred == 1:
    print("We predict an EXIT of the client")
else:
    print("We predict client will REMAIN in the bank")
	
	
	
	
	
	
	


bagging_score(b_sample, df, 'Exited')








df












from sklearn.ensemble import RandomForestClassifier
X = df.drop(['Exited'], axis=1)
y = df['Exited']
random_forest = RandomForestClassifier(n_estimators = 3, bootstrap= True).fit(X, y)
y = random_forest.predict_proba([[500, 0, 1, 40, 1, 5000, 1, 1, 0, 100000]])
y








</pre></td>
  <tr>
    <th>Company 8 ML</th>
    
  </tr>
practica 8 
<td><pre>

<h4>AdaBoost y usamos libreria AdaBoostClassifier de SKLearn </h4>



 anexo de Excel como es el calculo de los disitntos parametros necesarios para generar los clasificadores débiles.
 
 
 
 
 
 


# librerias a utilizar
import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
import matplotlib.pyplot as plt






df = pd.read_csv('churn_cleaned.csv')
df = df[['CreditScore', 'Balance', 'Exited']]
df









plt.scatter(df['CreditScore'], df['Balance'], c=df['Exited'])
plt.xlabel('Balance')
plt.ylabel('Credit Score')
plt.text(900, 5, "Yellow: 1, Violet: 0")
plt.show()







clf = AdaBoostClassifier(n_estimators=5, random_state=0)











X = df[['CreditScore', 'Balance']]
y = df[['Exited']]
clf.fit(X, y)











# Que usamos para las estimaciones ?
clf.base_estimator_














# Cada arbol
clf.estimators_













from sklearn import tree
d_tree = tree.DecisionTreeClassifier(max_depth=1, random_state=1537364731).fit(X,y)
import matplotlib.pyplot as plt
plt.figure(figsize=(6,6))
tree.plot_tree(d_tree, fontsize=10)
plt.show()








# forma de la variable clasificatoria
clf.classes_







# Suma de Alpha * valor
clf.decision_function([[619, 0]])









# Predicion -acierta
clf.predict([[619, 0]])







# Suma de Alpha * Valor
clf.decision_function([[822, 0]])






# Predicion -acierta
clf.predict([[822, 0]])







clf.score(X, y)


</pre></td>
<tr>
    <th>Company 9 ML</th>
    
  </tr>
practica 9
<td><pre>


<h4>implementar modelos de aprendizaje no supervizado: K-Means & Hierarchical Clustering.</h4>


Autoria del tutorial: https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c







Para esta practica crearemos un toy dataset. Utilizaremos make_blobs 
ya que permite la generación de "gotas" (clusters) de datos con distribución gausiana e isotrópica.











import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# create dataset
X, y = make_blobs(
   n_samples=150, n_features=2,
   centers=3, cluster_std=0.5,
   shuffle=True, random_state=0
)

# plot
plt.scatter(
   X[:, 0], X[:, 1],
   c='white', marker='o',
   edgecolor='black', s=50
)
plt.show()










Para ver la documentacion completa: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans














from sklearn.cluster import KMeans

km = KMeans(
    n_clusters=3, init='random',
    n_init=10, max_iter=300, 
    tol=1e-04, random_state=0
)
y_km = km.fit_predict(X)











# 3 clusters
# it will run 10 times independently with random centroid seeds
# max_iter number of iterations for every run









X












km.cluster_centers_













y_km











y_km == 0, 0















y_km == 0, 1



















# plot the 3 clusters
plt.scatter(
    X[y_km == 0, 0], X[y_km == 0, 1],
    s=50, c='lightgreen',
    marker='s', edgecolor='black',
    label='cluster 1'
)

plt.scatter(
    X[y_km == 1, 0], X[y_km == 1, 1],
    s=50, c='orange',
    marker='o', edgecolor='black',
    label='cluster 2'
)

plt.scatter(
    X[y_km == 2, 0], X[y_km == 2, 1],
    s=50, c='lightblue',
    marker='v', edgecolor='black',
    label='cluster 3'
)

# plot the centroids
plt.scatter(
    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],
    s=250, marker='*',
    c='red', edgecolor='black',
    label='centroids'
)
plt.legend(scatterpoints=1)
plt.grid()
plt.show()



















km.predict([[1.5,2.8]])











import pandas as pd
df = pd.read_csv('churn_cleaned.csv')
df







X = df[['CreditScore', 'EstimatedSalary']]
X = X.sample(n=150, random_state=1)











plt.scatter(
   X['CreditScore'],X['EstimatedSalary'],
   c='white', marker='o',
   edgecolor='black', s=50
)
plt.show()












km = KMeans(
    n_clusters=2, init='random',
    n_init=10, max_iter=300, 
    tol=1e-04, random_state=0
)
y_km = km.fit_predict(X)









X








y_km












# plot the 2 clusters
plt.scatter(
    X[y_km == 0]['CreditScore'], X[y_km == 0]['EstimatedSalary'],
    s=50, c='lightgreen',
    marker='s', edgecolor='black',
    label='cluster 1'
)

plt.scatter(
    X[y_km == 1]['CreditScore'], X[y_km == 1]['EstimatedSalary'],
    s=50, c='orange',
    marker='o', edgecolor='black',
    label='cluster 2'
)
# plot the centroids
plt.scatter(
    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],
    s=250, marker='*',
    c='red', edgecolor='black',
    label='centroids'
)
plt.xlabel("CreditScore")
plt.ylabel("EstimatedSalary")
plt.legend(scatterpoints=1)
plt.grid()
plt.show()









# calculate distortion for a range of number of cluster
distortions = []
for i in range(1, 11):
    km = KMeans(
        n_clusters=i, init='random',
        n_init=10, max_iter=300,
        tol=1e-04, random_state=0
    )
    km.fit(X)
    distortions.append(km.inertia_)

# plot
plt.plot(range(1, 11), distortions, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.show()








#Como vemos, en este caso es conveniente usar K = 2






</pre></td>
  <tr>
    <th>Company 10 ML</th>
    
  </tr>		 
practica 10
<td><pre>




<h4> red neuronal o neural network</h4>








import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
import pandas as pd












df = pd.read_csv('churn_cleaned.csv')
df










df.isnull().sum()











y = df['Exited']
y










X = df.drop(['Exited'], axis=1)
X









x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20)










nn = MLPClassifier(
    hidden_layer_sizes=(2,2,2),
    activation = 'logistic',
    max_iter=10000
).fit(x_train, y_train)










y_pred = nn.predict(x_test)










confusion_mat = confusion_matrix(y_test,y_pred)
confusion_mat










accuracy = accuracy_score(y_test,y_pred)
accuracy






y_pp = nn.predict_proba(x_test)
y_pp







nn.coefs_










y_preds = []
for i in y_pp:
    if i[1] > 0.8:
        y_preds.append(1)
    else:
        y_preds.append(0)
y_preds









</pre></td>		  
<tr>
    <th>Company 11   RESUMEN GENERAL - ALGORITMOS</th>
    
  </tr>	 
<td><pre>


<h1> Algoritmos manuales para recordar </h1>



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm 
import operator
from sklearn.linear_model import LogisticRegression
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures
from sklearn import neighbors
from sklearn import tree
from sklearn.metrics import confusion_matrix
#LAS DUDOSAS QUE PROBABLEMENTE NO SE PERMITAN EN EL EXAMEN PERO ESTAN VISTAS ACA
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error







<h2>Algoritmo de MSE</h2>




def mean_squared_error_manual(y_reales, y_predicciones):
    reales = np.array(y_reales)
    predicciones = np.array(y_predicciones)
    diferencia = np.subtract(reales, predicciones)
    cuadrados = np.power(diferencia, 2)
    return cuadrados.mean()
	
	



<h2> Setup para algoritmos de seleccion como forward, backward y backward with p-values</h2>





dataframe = pd.read_csv('boston.csv')
variables_explicativas = dataframe.drop('MEDV', axis=1)
variable_objetivo = dataframe[['MEDV']]
variables_explicativas = sm.add_constant(variables_explicativas)
print(variables_explicativas.columns)





<h2>Algoritmo de Forward Stepwise Selection</h2>




def forward_stepwise_selection(variables_explicativas, variable_objetivo):
    variables = ['const']
    r2_adj = []
    var_model = {}
    iterate_columns = variables_explicativas.columns.drop('const', 1)
    vars_size = iterate_columns.size
    for k in range(0, vars_size):
        r2 = {}
        for var in iterate_columns:
            variable_explicativa = variables_explicativas[variables + [var]]
            model = sm.OLS(variable_objetivo, variable_explicativa)
            regr = model.fit()
            r2[var] = regr.rsquared_adj
        best_var = max(r2.items(), key=operator.itemgetter(1))[0]
        r2_adj.append(r2[best_var])
        variables.append(best_var)
        iterate_columns = iterate_columns.drop(best_var, 1)
        var_model[k] = best_var
    r2_max_index = r2_adj.index(max(r2_adj))
    return r2_max_index, var_model

r2_max_index, var_model = forward_stepwise_selection(variables_explicativas, variable_objetivo)
i = 0
variables = []
while i <= r2_max_index:
    variables.append(var_model[i])
    i += 1
variables_explicativa = variables_explicativas[variables]
sm.add_constant(variables_explicativa)
model = sm.OLS(variable_objetivo, variables_explicativa)
regr = model.fit()
print(regr.summary())









<h2>Algoritmo de Backward Stepwise Selection</h2>





def bacward_stepwise_selection(variables_explicativas, variable_objetivo):
    variables = variables_explicativas.columns
    iterate_columns = variables.drop('const', 1)
    vars_size = iterate_columns.size
    r2_adj = []
    var_model = {}
    for k in range(0, vars_size):
        r2 = {}
        i = 1
        for var in iterate_columns:
            variables = variables.drop(var, 1)
            variable_explicativa = variables_explicativas[variables]
            variables = variables.insert(i, var)
            model = sm.OLS(variable_objetivo, variable_explicativa)
            regr = model.fit()
            r2[var] = regr.rsquared_adj
            i += 1
        r2_max_var = max(r2.items(), key=operator.itemgetter(1))[0]
        r2_adj.append(r2[r2_max_var])
        variables = variables.drop(r2_max_var, 1)
        var_model[k] = r2_max_var
        iterate_columns = iterate_columns.drop(r2_max_var, 1)
    r2_max_index = r2_adj.index(max(r2_adj))
    return r2_max_index, var_model

r2_max_index, var_model = bacward_stepwise_selection(variables_explicativas, variable_objetivo)
i = 0
variables = variables_explicativas.columns
while i <= r2_max_index:
    variables = variables.drop(var_model[i], 1)
    i += 1
variables_explicativa = variables_explicativas[variables]
sm.add_constant(variables_explicativa)
model = sm.OLS(variable_objetivo, variables_explicativa)
regr = model.fit()
print(regr.summary())








<h2> Algoritmo de Seleccion Backward Stepwise con p-values</h2>





def bacward_stepwise_selection_pvalues(variables_explicativas, variable_objetivo):
    variables = variables_explicativas.columns
    r2_adj = []
    var_model = {}
    vars_size = variables.drop('const', 1).size
    for k in range(0, vars_size):
        variable_explicativa = variables_explicativas[variables]
        model = sm.OLS(variable_objetivo, variable_explicativa)
        regr = model.fit()
        p_values = regr.pvalues
        p_values = p_values.drop('const')
        p_value_index = p_values.argmax()
        p_value_var = p_values.keys()[p_value_index]
        variables = variables.drop(p_value_var, 1)
        r2_adj.append(regr.rsquared_adj)
        var_model[k] = p_value_var
    r2_max_index = r2_adj.index(max(r2_adj))
    return r2_max_index, var_model

r2_max_index, var_model = bacward_stepwise_selection_pvalues(variables_explicativas, variable_objetivo)
i = 0
variables = variables_explicativas.columns
while i <= r2_max_index:
    variables = variables.drop(var_model[i], 1)
    i += 1
variables_explicativa = variables_explicativas[variables]
sm.add_constant(variables_explicativa)
model = sm.OLS(variable_objetivo, variables_explicativa)
regr = model.fit()
print(regr.summary())




<h2> Creacion de variables dummies </h2>




def add_dummies(dummy_columns, dataframe):
    for dummy in dummy_columns:
        dataframe = add_dummy(dataframe, dummy)
    return dataframe

def add_dummy(df, dummy_column_name):
    dummies = pd.get_dummies(df[dummy_column_name], prefix=dummy_column_name)
    index = df.columns.get_loc(dummy_column_name)
    df = df.drop(dummy_column_name, 1)
    dummies_columns = list(dummies.columns)
    for dummy in dummies_columns:
        df.insert(index, dummy, dummies[[dummy]])
        index += 1
    return df
	




<h2> Metodo de Cross - Validation K-fold </h2>







dataframe = pd.read_csv('boston.csv')
df_random = dataframe.sample(frac=1, ignore_index=True)
size = dataframe.shape[0]
kfolds = 5

def cross_val_score_manual_v2(dataframe, model, var_explicativas, var_objetivo, kfold=5):
    scores = np.array([])
    size = dataframe.shape[0]
    interval = np.linspace(start=0, stop=size-1, num=kfold+1)
    for i in range(kfold):
        test = dataframe.iloc[int(interval[i]):int(interval[i+1])]
        train = dataframe.drop(test.index)
        X_train, Y_train = train[var_explicativas], train[var_objetivo]
        X_test, Y_test = test[var_explicativas], test[var_objetivo]
        regr = model.fit(X_train, Y_train)
        Y_predict = regr.predict(X_test)
        #scores = np.append(scores, mean_squared_error(Y_test, Y_predict))
    scores = np.append(scores, mean_squared_error(Y_test, Y_predict))
    print(scores)
    return scores.mean()

modelos = {}
modelo1 = linear_model.LinearRegression()
scores = cross_val_score_manual_v2(df_random, modelo1, ['LSTAT'], ['MEDV'], 5)
modelos["modelo1"] = scores
modelo2 = linear_model.LinearRegression()
scores = cross_val_score_manual_v2(df_random, modelo2, ['RM'], ['MEDV'], 5)
modelos["modelo2"] = scores
modelo3 = linear_model.LinearRegression()
scores = cross_val_score_manual_v2(df_random, modelo3, ['LSTAT', 'RM'], ['MEDV'], 5)
modelos["modelo3"] = scores
best_model = min(modelos.items(), key=operator.itemgetter(1))[0]
best_model









<h2> Algoritmo de Accuracy </h2>







def accuracy(dataframe, target, model, threshold=0.5):
    y = dataframe[target]
    x = dataframe.drop(target, axis=1)
    difference = list(y - predict_manual(model, x, threshold))
    count = 0
    for i in range(0, len(difference)):
        if difference[i] == 0:
            count += 1
    accuracy = count / len(difference)
    return accuracy








<h2> Algortimos de matriz de confusion y regresion logistica </h2>








df_categorical = pd.read_csv('Churn_Modelling.csv')
df_categorical = df_categorical.drop(['RowNumber', 'CustomerId', 'Surname', 'Geography', 'Gender'], axis=1)
variables_explicativas = df_categorical.drop(['Exited'], axis=1)
variable_objetivo = df_categorical[['Exited']]
model = LogisticRegression()
regr = model.fit(variables_explicativas, variable_objetivo)










def predict_manual(model, variables_explicativas, threshold=0.5):
    y_prob = model.predict_proba(variables_explicativas)
    y_pred = np.array([])
    for pro in y_prob:
        if pro[1] >= threshold:
            y_pred = np.append(y_pred, 1)
        else:
            y_pred = np.append(y_pred, 0)
    y_pred = y_pred[:,None]
    return y_pred





predicciones = predict_manual(regr, variables_explicativas, 0)
print(mean_squared_error_manual(predicciones, variable_objetivo))
print(mean_squared_error(predicciones, variable_objetivo))
print(mean_squared_error(regr.predict(variables_explicativas), variable_objetivo))








print("               PRED            ")
print("         0              1      ")
print("                 |             ")
print("R  0     TN      |      FP     ")
print("E                |             ")
print("A  --------------|-------------")
print("L                |             ")
print("   1     FN      |      TP     ")
print("                 |             ")







def print_confusion_matrix(Y, Y_pred):
    tn, fp, fn, tp = confusion_matrix(Y, Y_pred).ravel()
    print(pd.DataFrame({"0": [tn, fn], "1": [fp, tp]}), end='\n\n')
	
	
	
	
	
	




print_confusion_matrix(variable_objetivo, predicciones)








def variacion_threshold(model, Y):
    threshold = np.linspace(start=0, stop=0.99, num=99)
    for thr in threshold:
        Y_pred = predict_manual(regr, variables_explicativas, thr)
        print(f"Threshold= {thr}\n")
        print_confusion_matrix(Y, Y_pred)
		






variacion_threshold(regr, variable_objetivo)










<h2> Calculo de mejor esperanza </h2>




matriz_beneficio = np.array([[0,-1],[0, 99]])
print(matriz_beneficio)




def mejor_beneficio(model, Y, matriz_beneficio):
    calculos = {}
    threshold = np.linspace(start=0, stop=0.99, num=99)
    for thr in threshold:
        Y_pred = predict_manual(model, variables_explicativas, thr)
        print(f"Threshold= {thr}\n")
        print_confusion_matrix(Y, Y_pred)
        tn, fp, fn, tp = confusion_matrix(Y, Y_pred).ravel()
        total = tn + fp + fn + tp
        calculo = (tp/total) * matriz_beneficio[1][1] + (fp/total) * matriz_beneficio[0][1]
        calculos[thr] = calculo
        print(f"Calculo={calculo}\n")
    mejor_beneficio = max(calculos.items(), key=operator.itemgetter(1))
    return mejor_beneficio






matriz_beneficio = np.array([[0,-1],[0, 99]])
mejor_beneficio = mejor_beneficio(regr, variable_objetivo, matriz_beneficio)





mejor_beneficio


</pre></td>
<tr>
    <th>Company 12   S</th>
    
  </tr>	 
		 
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
		  
  <tr>
    <td></td>
  </tr>

</table>
    </div>

  <footer>
    <p align="center"><img src="images/logounb.png" alt="h" width="107" height="69"> Alcalde: Javier Antonio Pérez Tel. (503)23792517 Barrio El Centro|  Alcaldia de </a>San Emigdio </a><img src="images/ku.png" alt="t" width="101" height="90"></p>
  </footer>
	
</div>
  <p>&nbsp;</p>
  <!-- javascript at the bottom for fast page loading -->
  <script type="text/javascript" src="js/jquery.js"></script>
  <script type="text/javascript" src="js/jquery.easing-sooper.js"></script>
  <script type="text/javascript" src="js/jquery.sooperfish.js"></script>
  <script type="text/javascript">
    $(document).ready(function() {
      $('ul.sf-menu').sooperfish();
    });
  </script>
</body>
</html>
